{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация алгоритма Advantage-Actor Critic (A2C) (вплоть до 10 баллов)\n",
    "\n",
    "#### дедлайн задания (сразу жёсткий): 26 апреля, 23:59 UTC+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа выполнена: Богданов Александр Б05-003."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе Вам предстоит реализовать алгоритм `Advantage Actor Critic`, обучаемый на батче из сред `Atari 2600`, работающих параллельно.\n",
    "\n",
    "Для начала будут использованы обёртки сред, реализованные в файле `atari_wrappers.py`. Эти обёртки предварительно обрабатывают наблюдения (производят преобразования размера, цвета фрейма, взятия максимума между фреймами, пропускают часть фреймов и сводят несколько фреймов в один большой) и вознаграждения. Некоторые обёртки помогают автоматически перезапустить среду и присвоить переменной `done` значение `True` в случае смерти агента. Файл `env_batch.py` включает в себя реализацию класса `ParallelEnvBatch`, позволяющего запускать несколько сред параллельно. Для создания (инициализации) среды можно воспользоваться функцией `nature_dqn_env`. Обратите внимание, что в случае использования `PyTorch` (https://pytorch.org/) без `tensorboardX` (https://github.com/lanpa/tensorboardX) потребуется самостоятельно реализовать обёртку среды, которая будет логрировать **исходные** суммарные награды, которые *исходная* среда возвращает, и переопределить реализацию функции `nature_dqn_env`. То есть настоятельно рекомендуется применить `tensorboardX`.\n",
    "\n",
    "Псевдокод алгоритма `Advantage Actor Critic (A2C)` приведён в **Разделе 5.2.5 (Алгоритм 20)** конспекта лекций: https://arxiv.org/pdf/2201.09746.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скрипты в данной работе используют Python версию библиотеки [OpenCV](https://pypi.org/project/opencv-python/). Для запуска сред ATARI понадобится библиотека [The Arcade Learning Environment](https://github.com/Farama-Foundation/Arcade-Learning-Environment), а также библиотека [Shimmy](https://pypi.org/project/Shimmy/). Может потребоваться установка `ffmpeg` кодека. Для Unix-based операционной системы с пакетным менеджером APT может потребоваться следующая последовательность команд:\n",
    "```\n",
    "sudo apt-get install -y xvfb x11-utils ffmpeg python-opengl\n",
    "pip install pyglet pyvirtualdisplay opencv-python tqdm numpy moviepy gymnasium[atari]\n",
    "pip install gymnasium[accept-rom-license] #run 'pip install autorom; AutoROM --accept-license' if this fails\n",
    "```\n",
    "Для MacOS систем рекомендуется установить репозиторий [TFM](https://github.com/serrodcal-MII/TFM/tree/main) и выполнить инструкции из [README.md](https://github.com/serrodcal-MII/TFM/blob/main/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:24:54.694352Z",
     "start_time": "2024-04-19T13:24:54.044078Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "from atari_wrappers import nature_dqn_env\n",
    "from runners import EnvRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:24:55.387113Z",
     "start_time": "2024-04-19T13:24:55.243895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: ../xvfb: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# XVFB будет запущен в случае исполнения на сервере\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:24:57.784375Z",
     "start_time": "2024-04-19T13:24:55.750210Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "nenvs = 12\n",
    "\n",
    "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=nenvs, summaries = \"Tensorboard\")\n",
    "                                                    # nenvs -- количество параллельно запущенных сред\n",
    "                                                    # данный параметр можно варьировать для баланса\n",
    "                                                    # производительность итерации/надёжность Монте-Карло оценок\n",
    "                                                    # помните: при уменьшении nenvs, возможно, придётся\n",
    "                                                    # увеличить количество итераций оптимизации\n",
    "n_actions = env.action_space.spaces[0].n\n",
    "obs, _ = env.reset()\n",
    "assert obs.shape == (nenvs, 4, 84, 84)\n",
    "assert obs.dtype == np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующим шагом будет реализация модели, которая выводит логиты для категориального распределения на действия и оценку на значения $V$-функции ценности. Рекомендуется использовать архитектуру модели, представленной в публикации в журнале [Nature](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) со следующей модификацией: вместо одного выходного слоя нужно сделать два слоя, принимающих в качестве входа выход предшествующего скрытого слоя. **Обратите внимание**, данная модель отличается от модели, предложенной в домашней работе по DQN. Рекомендуется использовать ортогональную инициализацию с параметром $\\sqrt{2}$ для ядер свёрток и инициализировать смещения нулями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:24:59.048476Z",
     "start_time": "2024-04-19T13:24:59.036516Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv2d_size_out(size, kernel_size, stride):\n",
    "    return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "class NatureModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, bsize, n_channels, height, width, n_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(n_channels, 32, kernel_size=8, stride=4)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        \n",
    "        self.conv3 = torch.nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        \n",
    "        h, w = conv2d_size_out(height, 8, 4), conv2d_size_out(width, 8, 4)\n",
    "        h, w = conv2d_size_out(h, 4, 2), conv2d_size_out(w, 4, 2)\n",
    "        h, w = conv2d_size_out(h, 3, 1), conv2d_size_out(w, 3, 1)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(64 * h * w, 512)\n",
    "        self.relu4 = torch.nn.ReLU()\n",
    "        \n",
    "        self.fc_logits = torch.nn.Linear(512, n_actions)\n",
    "        self.fc_values = torch.nn.Linear(512, 1)\n",
    "        \n",
    "        self._reset_parameters()\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        torch.nn.init.orthogonal_(self.conv1.weight.data, gain=np.sqrt(2))\n",
    "        self.conv1.bias.data.fill_(0.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.conv2.weight.data, gain=np.sqrt(2))\n",
    "        self.conv2.bias.data.fill_(0.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.conv3.weight.data, gain=np.sqrt(2))\n",
    "        self.conv3.bias.data.fill_(0.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.fc1.weight.data, gain=np.sqrt(2))\n",
    "        self.fc1.bias.data.fill_(0.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.fc_logits.weight.data, gain=np.sqrt(2))\n",
    "        self.fc_logits.bias.data.fill_(0.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.fc_values.weight.data, gain=np.sqrt(2))\n",
    "        self.fc_values.bias.data.fill_(0.0)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.tensor(state, dtype=torch.float)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        logits = self.fc_logits(x)\n",
    "        value = self.fc_values(x)\n",
    "        \n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам также потребуется определить и использовать политику, которая будет использовать модель выше. В то время как модель вычисляет логиты для всех действий сразу и оценку функции ценности, политика будет сэмплировать действия, а также будет вычислять их логарифм правдоподобия. Метод `Policy.act` должен возвращать словарь всех массивов, требуемых для взаимодействия со средой и обучения модели. Обратите внимание, что действия должны быть формата `Numpy.ndarray`, в то время как другие тензоры должны быть в формате, определяемом библиотекой глубокого обучения (`Torch.tensor`, например)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:24:59.992627Z",
     "start_time": "2024-04-19T13:24:59.987617Z"
    }
   },
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def act(self, inputs):\n",
    "        logits, value = self.model(inputs)\n",
    "        m = Categorical(logits=logits)\n",
    "        action = m.sample()\n",
    "        return {'actions': np.array(action.data),\n",
    "                'logits': logits,\n",
    "                'log_prob': m.log_prob(action).view(-1, 1),\n",
    "                'values': value}\n",
    "    \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее требуется передать среду и политику в исполнитель `EnvRunner`, который собирает частичные траектории из среды. Класс `EnvRunner` уже реализован за Вас.\n",
    "\n",
    "Данный исполнитель взаимодействует со средой заданное количество шагов и возвращает словарь, содержащий ключи:\n",
    "\n",
    "* 'observations' \n",
    "* 'rewards' \n",
    "* 'resets'\n",
    "* 'actions'\n",
    "* и другие ключи, определённые в `Policy`\n",
    "\n",
    "по каждому из этих ключей содержится Python `list` соответствующих результатов взаимодействий со средой указанной длины $T$ &mdash; размера частичной траектории."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы обучить часть модели, которая предсказывает ценности состояний, требуется вычислить целевые значения ценностей. В инстанцию класса `EnvRunner` можно подать при создании по аргументу `transforms` список вызываемых объектов (\"функций\"), которые последовательно будут применяться к частичным траекториям после сбора самих траекторий. Следовательно, требуется реализовать и использовать вызываемый (с определённым методом `__call__`) класс `ComputeValueTargets`. Формула для вычисления целевых значений ценности простая:\n",
    "\n",
    "$$\n",
    "\\hat v(s_t) = \\sum_{t'=0}^{T - 1}\\gamma^{t'}r_{t+t'} + \\gamma^T \\hat{v}(s_{t+T}),\n",
    "$$\n",
    "\n",
    "Однако, не забудьте в реализации использовать `trajectory['resets']` флаги для проверки того, следует ли добавить целевые значения ценности на следующем шаге при вычислении целевых значений ценности на текущем шаге. У вас также имеется доступ к `trajectory['state']['latest_observation']` для получения последнего наблюдения в частичной траектории &mdash; $s_{t+T+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:25:01.525540Z",
     "start_time": "2024-04-19T13:25:01.518305Z"
    }
   },
   "outputs": [],
   "source": [
    "class ComputeValueTargets:\n",
    "    def __init__(self, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, trajectory):\n",
    "        value_targets = []\n",
    "        trajectory['rewards'][-1] = torch.Tensor(trajectory['rewards'][-1]).view(-1, 1)\n",
    "        trajectory['resets'][-1] = torch.Tensor(trajectory['resets'][-1]).view(-1, 1)\n",
    "        value_targets.append((1 - trajectory['resets'][-1]) * trajectory['values'][-1])\n",
    "        for i in range(len(trajectory['rewards']) - 2, -1, -1):\n",
    "            trajectory['rewards'][i] = torch.Tensor(trajectory['rewards'][i]).view(-1, 1)\n",
    "            trajectory['resets'][i] = torch.Tensor(trajectory['resets'][i]).view(-1, 1)\n",
    "            value_targets.append((1 - trajectory['resets'][i]) * (1 - trajectory['resets'][i + 1]) * (\n",
    "                                 trajectory['rewards'][i] + self.gamma * value_targets[-1]) +\\\n",
    "                                 trajectory['resets'][i + 1] * (1 - trajectory['resets'][i]) *\\\n",
    "                                 trajectory['values'][i])\n",
    "        trajectory['value_targets'] = []\n",
    "        trajectory['value_targets'].extend(reversed(value_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После вычисления целевых значений ценности требуется преобразовать списки результатов взаимодействия со средой в тензоры с первой компонентой размерности `batch_size`, равной призведению `T * nenvs`, то есть требуется свести в одну компоненту первые две компоненты размерности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:25:02.364629Z",
     "start_time": "2024-04-19T13:25:02.359029Z"
    }
   },
   "outputs": [],
   "source": [
    "class MergeTimeBatch:\n",
    "    \"\"\"\n",
    "    Сращивает первые две оси, обычно отвечающие за время и за инстанцию среды, соответственно.\n",
    "    \"\"\"\n",
    "    def __call__(self, trajectory):\n",
    "        for k, v in trajectory.items():\n",
    "            if k in ['value_targets', 'log_prob', 'values', 'logits']:\n",
    "                t = torch.stack(v, dim=0)\n",
    "                trajectory[k] = t.reshape(-1, *t.shape[2:])\n",
    "    \n",
    "        return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:25:02.958990Z",
     "start_time": "2024-04-19T13:25:02.867122Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 617 # на своё усмотрение можно выбрать другой seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = NatureModel(*obs.shape, n_actions)\n",
    "policy = Policy(model)\n",
    "runner = EnvRunner(env, policy, nsteps=6, # nsteps -- длина частичной траектории\n",
    "                                          # уменьшение nsteps может привести к вынужденному увеличению\n",
    "                                          # количества итераций оптимизации\n",
    "                   transforms=[ComputeValueTargets(),\n",
    "                               MergeTimeBatch()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настало время реализовать сам алгоритм Advantage-Actor Critic (A2C). Его псевдокод можно посмотреть в [конспектах лекций (Раздел 5.2.5)](https://arxiv.org/pdf/2201.09746.pdf), в публикции [Mnih et al. 2016](https://arxiv.org/abs/1602.01783) и в [лекции](https://www.youtube.com/watch?v=Tol_jw5hWnI&list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&index=20) Сергея Левина."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:25:04.056178Z",
     "start_time": "2024-04-19T13:25:04.049788Z"
    }
   },
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    def __init__(self,\n",
    "               policy,\n",
    "               optimizer,\n",
    "               value_loss_coef=.25,\n",
    "               entropy_coef=1e-2,\n",
    "               max_grad_norm=.5):\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def policy_loss(self, trajectory):\n",
    "        m = Categorical(logits=trajectory['logits'])\n",
    "        advantages = trajectory['value_targets'] - trajectory['values']\n",
    "        policy_loss = -(trajectory['log_prob'] * advantages.detach()).mean() - self.entropy_coef * m.entropy().mean()\n",
    "        return policy_loss\n",
    "    \n",
    "    def value_loss(self, trajectory):\n",
    "        value_loss = ((trajectory['value_targets'].detach() - trajectory['values']) ** 2).mean()\n",
    "        return value_loss\n",
    "\n",
    "    def loss(self, trajectory):\n",
    "        loss = self.policy_loss(trajectory) + self.value_loss_coef * self.value_loss(trajectory)       \n",
    "        return loss\n",
    "\n",
    "    def step(self, trajectory):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss(trajectory)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy.model.parameters(), self.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "        loss_value = loss.item()\n",
    "        return loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно непосредственно обучить Вашу модель. С разумно подобранными гиперпараметрами обучение на одной GTX1080 на протяжении 10 миллионов шагов суммарно со всех батчированных сред (что переводится примерно в 5 часов работы) должно быть возможно достигнуть *среднюю исходную награду за 100 последних эпизодов* (значение переменной в `Tensorboard` по ключу `reward_mean_100`, усреднение берётся по 100 последним эпизодам в каждой среде в батче) **не меньше 600**. Это и будет считаться успешным результатом работы алгоритма `A2C`.\n",
    "\n",
    "**Внимание!** При *коректной* имплементации алгоритма *обоснованное* преодоление порога **400** по `reward_mean_100` *транслируется в* **8 баллов за задание**, *обоснованное* преодоление порога **600** по `reward_mean_100` на *корректно* написанном алгоритме обучения `A2C` *транслируется в* **10 баллов за задание**.\n",
    "\n",
    "Вам так же, по возможности, рекомендуется отобразить данную величину относительно `runner.step_var` &mdash; количества взаимодействий со всеми средами. Также очень рекомендуется предоставить графики следующих показателей (полезно для отладки кода):\n",
    "* [Коэффициент детерминации](https://en.wikipedia.org/wiki/Coefficient_of_determination) между целевыми значениями ценности и их предсказаниями\n",
    "* Энтропия политики $\\pi$\n",
    "* Функция потерь ценности (Value loss)\n",
    "* Функция потерь политики (Policy loss)\n",
    "* Целевые значения ценности (Value targets)\n",
    "* Предсказания значений ценности (Value predictions)\n",
    "* Норма градиента\n",
    "* Advantages\n",
    "* Общая функция потерь (A2C loss)\n",
    "\n",
    "В качестве оптимизатора рекомендуется взять метод [RMSProp](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html) с [линейным убыванием шага](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html), начиная с 7e-4 до 0, константой сглаживания (alpha в PyTorch и decay в TensorFlow), равной 0.99 и epsilon, равным 1e-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T01:43:12.609689Z",
     "start_time": "2024-04-16T01:43:12.605243Z"
    }
   },
   "source": [
    "Запуск Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:29:48.333868Z",
     "start_time": "2024-04-19T13:29:47.291512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fda595d4196f0cf3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fda595d4196f0cf3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:25:34.595031Z",
     "start_time": "2024-04-19T13:25:34.092307Z"
    }
   },
   "outputs": [],
   "source": [
    "n_steps = 350000\n",
    "\n",
    "runner.reset()\n",
    "optimizer = opt.RMSprop(policy.model.parameters(), lr=7e-4, eps=1e-5)\n",
    "scheduler = opt.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda x: (1.0 - x / n_steps))\n",
    "a2c = A2C(policy, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:28:05.548598Z",
     "start_time": "2024-04-19T13:28:05.545735Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(n_steps), desc='Step'):\n",
    "    trajectory = runner.get_next()\n",
    "    a2c.step(trajectory)\n",
    "    scheduler.step()\n",
    "    if i % 49999 == 0:\n",
    "        torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:32:45.565417Z",
     "start_time": "2024-04-19T13:32:45.484795Z"
    }
   },
   "outputs": [],
   "source": [
    "model = NatureModel(*obs.shape, n_actions)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "policy = Policy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:38:15.882455Z",
     "start_time": "2024-04-19T13:38:15.822039Z"
    }
   },
   "outputs": [],
   "source": [
    "test_env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=None, clip_reward=False, summaries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:38:18.149047Z",
     "start_time": "2024-04-19T13:38:18.143099Z"
    }
   },
   "outputs": [],
   "source": [
    "n_lives = 3\n",
    "\n",
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\"\n",
    "    Играем n_games игр до конца.\n",
    "    В случае жадной политики, выбираем действия как argmax(qvalues).\n",
    "    Возвращаем среднюю награду.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            output = agent.act([s])\n",
    "            action = output['logits'].argmax(dim=-1).item() if greedy else output['actions'][0]\n",
    "            s, r, terminated, truncated, _ = env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:38:21.535424Z",
     "start_time": "2024-04-19T13:38:19.139235Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksandrbogdanov/anaconda3/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/False folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/aleksandrbogdanov/anaconda3/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/False/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/False/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/False/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/False/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/False/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/False/rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/False/rl-video-episode-2.mp4.\n",
      "Moviepy - Writing video /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/False/rl-video-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/False/rl-video-episode-2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# запись эпизодов\n",
    "with RecordVideo(\n",
    "    env=test_env,\n",
    "    video_folder=\"./videos/False\",\n",
    "    episode_trigger=lambda episode_number: True\n",
    ") as env_monitor:\n",
    "    sessions = [evaluate(env_monitor, policy, n_games=n_lives, greedy=False) for _ in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:44:04.046625Z",
     "start_time": "2024-04-19T13:44:04.042337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos/False/rl-video-episode-0.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos', 'False').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[0]\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:38:44.357438Z",
     "start_time": "2024-04-19T13:38:36.994926Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksandrbogdanov/anaconda3/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/True folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/True/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/True/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/True/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/True/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/True/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/True/rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/True/rl-video-episode-2.mp4.\n",
      "Moviepy - Writing video /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/True/rl-video-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/aleksandrbogdanov/Учеба/8 Семестр/RL/Задание 4/videos/True/rl-video-episode-2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# запись эпизодов\n",
    "with RecordVideo(\n",
    "    env=test_env,\n",
    "    video_folder=\"./videos/True\",\n",
    "    episode_trigger=lambda episode_number: True\n",
    ") as env_monitor:\n",
    "    sessions = [evaluate(env_monitor, policy, n_games=n_lives, greedy=True) for _ in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:38:53.678209Z",
     "start_time": "2024-04-19T13:38:53.670164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos/True/rl-video-episode-0.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos', 'True').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[0]\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Информация об обучении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прикрепление скриншотов графиков обучения модели в `Tensorboard` ниже является обязательным. Для доступа к `Tensorboard` запустите из командной строки в одной директории с данным ноутбуком следующую команду:\n",
    "```\n",
    "tensorboard --logdir logs --port 6006\n",
    "```\n",
    "В результате вывод в командную строку укажет, по какому адресу можно подсоединиться к инстанции `Tensorboard`, например, по адресу `http://localhost:6006/`. Оттуда можно и сделать скриншоты, демонстрирующие результаты обучения модели. Сами скриншоты с именем файла `image_name_x.png` для удобства лучше сохранить в директорию `./img`, откуда можно легко их прикреплять в `Markdown-клетках` ниже по команде со следующей конструкцией:\n",
    "```\n",
    "<img src=./img/image_name_x.png width=640>\n",
    "```\n",
    "Тут также требуется подписать изображения и дать небольшой комментарий по каждому скриншоту, что на нём описано.\n",
    "\n",
    "**Внимание!** В случае перезапуска процедуры обучения модели рекомендуется удалить директорию `./logs` вместе с её содержимым перед непосредственным перезапуском, чтобы не испортить отображающиеся графики в `Tensorboard`.\n",
    "\n",
    "**Совет.** При работе в Google Colab можно просто скачать директорию `./logs` и уже локально запустить `Tensorboard` для снятия скриншотов. Также можно обученного агента сохранить, скачать и локально на cpu запустить для записи роликов (для этого понадобится самостоятельно прописать код сохранения и загрузки модели в ноутбук из [файла](https://pytorch.org/tutorials/beginner/saving_loading_models.html)).\n",
    "\n",
    "**Внимание!** Посылку для сдачи задания требуется оформить в виде `.zip` архива, в котором будут *данный ноутбук*, использованные для его работы *скрипты*, *директории* `./videos`, `./logs` и `./img` с содержимым. Только так и не иначе!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./img/reward_mean_100.png width=640>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
